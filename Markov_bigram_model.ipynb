{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Markov bigram model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOPWEJBgbtMojBKlFpNw1eo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dvircohen0/NLP/blob/main/Markov_bigram_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhg6MfIbnmQG",
        "outputId": "0d098249-c05d-4fc4-d35e-b81425d604a3"
      },
      "source": [
        "import re\r\n",
        "import numpy as np\r\n",
        "from collections import Counter\r\n",
        "import pandas as pd\r\n",
        "import nltk\r\n",
        "nltk.download('brown')\r\n",
        "from nltk.corpus import brown"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3nenKRcn0tE"
      },
      "source": [
        "Define the vocabulary class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRVKLAa1nsb3"
      },
      "source": [
        "class Vocabulary:\r\n",
        "    def __init__(self, name):\r\n",
        "        self.name = name\r\n",
        "        self.word2index = {}\r\n",
        "        self.word2count = {}\r\n",
        "        self.index2word = {}\r\n",
        "        self.num_words = 0\r\n",
        "        self.num_sentences = 0\r\n",
        "        self.longest_sentence = 0\r\n",
        "        self.smoothing = 0.001\r\n",
        "\r\n",
        "    def add_word(self, word):\r\n",
        "        if word not in self.word2index:\r\n",
        "            # First entry of word into vocabulary\r\n",
        "            self.word2index[word] = self.num_words\r\n",
        "            self.word2count[word] = 1\r\n",
        "            self.index2word[self.num_words] = word\r\n",
        "            self.num_words += 1\r\n",
        "        else:\r\n",
        "            # Word exists; increase word count\r\n",
        "            self.word2count[word] += 1\r\n",
        "            \r\n",
        "    def add_sentence(self, sentence):\r\n",
        "        for word in sentence:\r\n",
        "            if word:\r\n",
        "                self.add_word(word)\r\n",
        "        self.num_sentences += 1\r\n",
        "\r\n",
        "    def to_word(self, index):\r\n",
        "        return self.index2word[index]\r\n",
        "\r\n",
        "    def to_index(self, word):\r\n",
        "        return self.word2index[word]\r\n",
        "\r\n",
        "    #create bigrams for a givn sentence\r\n",
        "    def sentence_bigrams(self,sentence):\r\n",
        "        return zip(sentence, sentence[1:])\r\n",
        "\r\n",
        "    #create probabilities for bigrams in corpus\r\n",
        "    def create_prob_matrix(self,corpus):\r\n",
        "        counts = Counter()\r\n",
        "        self.prob_matrix={}\r\n",
        "        #counts all the bigrams\r\n",
        "        for sent in corpus:\r\n",
        "            counts.update(self.sentence_bigrams(list(map(str.lower,sent))))\r\n",
        "        #fill the probabilities with the log likelihood of each bigram\r\n",
        "        for bigr,cnt in counts.items():\r\n",
        "            # add 1 smoothing\r\n",
        "            prob=cnt+self.smoothing\r\n",
        "            # divide by count(A)+V \r\n",
        "            # A=the first word on the bigram \r\n",
        "            # V=size of the vocabulary\r\n",
        "            prob/=(self.word2count[bigr[0]]+voc.num_words)\r\n",
        "            # convert the probability to log likelihood\r\n",
        "            self.prob_matrix[bigr]=np.log(prob)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQkk3kKJn3uS"
      },
      "source": [
        "Checking probability of a new sentence "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBcVMfR0nuM_"
      },
      "source": [
        "#return a score for a given sentence. if the the score is high \r\n",
        "#the input is a true sentence with high probability\r\n",
        "def check_sentence(sentence):\r\n",
        "    #words = input(\"Enter a sentence:\\n\")\r\n",
        "    words = sentence.lower().split()\r\n",
        "    # check that all tokens exist in word2idx (otherwise, we can't get score)\r\n",
        "    bad_sentence = False\r\n",
        "    for token in words:\r\n",
        "      if token not in voc.word2index:\r\n",
        "        bad_sentence = True\r\n",
        "    if bad_sentence:\r\n",
        "              print(\"Sorry, you entered words that are not in the vocabulary\")\r\n",
        "    else:\r\n",
        "        # the first probability is the unigram of the first word in the sentence\r\n",
        "        sentence_prob = np.log(voc.word2count[words[0]]/sum(voc.word2count.values()))\r\n",
        "        for i in range(1,len(words)):\r\n",
        "            if (words[i-1],words[i]) in voc.prob_matrix:\r\n",
        "                sentence_prob += voc.prob_matrix[(words[i-1],words[i])]\r\n",
        "            else: sentence_prob += np.log(voc.smoothing/(voc.num_words+voc.word2count[words[i]]))\r\n",
        "        print(\"SCORE: \",sentence_prob/len(words))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s86lqyoNn7wb"
      },
      "source": [
        "Fill the vocabulary and the bigram probabilities matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIHoOZSynwDL"
      },
      "source": [
        "if __name__ == \"__main__\":\r\n",
        "    #read the courpus file, we gonna use the brown corpus\r\n",
        "    corpus = brown.sents()\r\n",
        "    #creat empty Vocabulary objct\r\n",
        "    voc = Vocabulary('test')\r\n",
        "    #fill the Vocabulary with the corpus file\r\n",
        "    for i,sent in enumerate(corpus):\r\n",
        "        voc.add_sentence(map(str.lower,sent))\r\n",
        "    #fill the probability matrix \r\n",
        "    voc.create_prob_matrix(corpus)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFAAoIkLn-Pl"
      },
      "source": [
        "Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYhXwj6ynxfU",
        "outputId": "996cb1fb-0ae3-4849-81bd-e03c5bec851f"
      },
      "source": [
        "    # check real sentence from the corpus:\r\n",
        "    sent=\"For the most part , this discussion will be confined to results \\\r\n",
        "    obtained since the introduction of the reference standard .\"\r\n",
        "    print(\"real sentence from the corpus: \",sent)\r\n",
        "    check_sentence(sent)\r\n",
        "    \r\n",
        "    # check real sentence from the wikipedia:\r\n",
        "    sent=\"Development of cat breeds started in the mid 19th century .\"\r\n",
        "    print(\"real sentence from the wikipedia: \",sent)\r\n",
        "    check_sentence(sent)\r\n",
        "    \r\n",
        "    # check fake sentence:\r\n",
        "    sent=\"black bear eats spaghetti\"\r\n",
        "    print(\"fake sentence: \",sent)\r\n",
        "    check_sentence(sent)    "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "real sentence from the corpus:  For the most part , this discussion will be confined to results obtained since the introduction of the reference standard .\n",
            "SCORE:  -7.899997360213471\n",
            "real sentence from the wikipedia:  Development of cat breeds started in the mid 19th century .\n",
            "SCORE:  -11.277622626562168\n",
            "fake sentence:  black bear eats spaghetti\n",
            "SCORE:  -15.456113905409566\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}