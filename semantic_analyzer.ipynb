{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPedXX0K0P3WrJTqroH6lo2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dvircohen0/NLP/blob/main/semantic_analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ouB6BnB7GbD",
        "outputId": "811087f2-ea61-4ebd-b53b-ce69905ffec4"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import svm\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import textwrap\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU6YIh8C0f1J",
        "outputId": "7080df05-ec6f-47e7-ee70-d56df1b8a38b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/AmirYunus/py_sentiment_analysis/9f0a92edc228d6af49f9bbeb66dcc80e8b412e29/electronics/positive.review\n",
        "!wget https://raw.githubusercontent.com/AmirYunus/py_sentiment_analysis/9f0a92edc228d6af49f9bbeb66dcc80e8b412e29/electronics/negative.review"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-19 08:57:34--  https://raw.githubusercontent.com/AmirYunus/py_sentiment_analysis/9f0a92edc228d6af49f9bbeb66dcc80e8b412e29/electronics/positive.review\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1105010 (1.1M) [text/plain]\n",
            "Saving to: ‘positive.review’\n",
            "\n",
            "positive.review     100%[===================>]   1.05M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-05-19 08:57:34 (23.7 MB/s) - ‘positive.review’ saved [1105010/1105010]\n",
            "\n",
            "--2021-05-19 08:57:34--  https://raw.githubusercontent.com/AmirYunus/py_sentiment_analysis/9f0a92edc228d6af49f9bbeb66dcc80e8b412e29/electronics/negative.review\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1113512 (1.1M) [text/plain]\n",
            "Saving to: ‘negative.review’\n",
            "\n",
            "negative.review     100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-05-19 08:57:35 (22.1 MB/s) - ‘negative.review’ saved [1113512/1113512]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqcnVSLI4pZF"
      },
      "source": [
        "positive_reviews=[]\n",
        "pos_rev =  BeautifulSoup(open('/content/positive.review').read())\n",
        "for p in pos_rev.findAll('review_text'):\n",
        "  positive_reviews.append(p.text.strip())\n",
        "\n",
        "negative_reviews =[]\n",
        "neg_rev =  BeautifulSoup(open('/content/negative.review').read())\n",
        "for n in neg_rev.findAll('review_text'):\n",
        "  negative_reviews.append(n.text.strip())\n",
        "\n",
        "np.random.shuffle(positive_reviews)\n",
        "positive_reviews = positive_reviews[:len(negative_reviews)]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9YrSoni2yeT",
        "outputId": "bb3645f6-b747-4346-d0d9-b8ac3b2ade69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pos_review = random.choice(positive_reviews)\n",
        "neg_review = random.choice(negative_reviews)\n",
        "print(\"positive review: \\n\")\n",
        "print(\"\\n\".join(textwrap.wrap(pos_review,70)) + \"\\n\")\n",
        "print(\"negatice review: \\n\")\n",
        "print(\"\\n\".join(textwrap.wrap(neg_review,70)))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive review: \n",
            "\n",
            "I was in the market for a new inkjet and finally settled on this\n",
            "printer. The photo output is awesome and it's very fast!  There are\n",
            "only two things I would comment on that new users might not be aware\n",
            "of. I was ready to return the printer as I thought it was defective\n",
            "until I found out what the problems were.  1) When the cartridges dry\n",
            "out, it will tell you that the cartridges are incompatible. This was\n",
            "confusing as I did have the correct cartridges in the printer. A\n",
            "\"replace cartridge\" message would have been less ambiguous.  2) The\n",
            "printer has so far appeared \"dead\" once. When I went to turn it on, it\n",
            "would not do so. The solution is on HP's website - you must remove the\n",
            "rear door and do a hard reset. I did this and the printer worked fine\n",
            "again.  The printer does not come with a USB cable, but those can be\n",
            "easily bought for $14-20.\n",
            "\n",
            "negatice review: \n",
            "\n",
            "I've had it for one week only and I'm using 3rd pair of Duracell\n",
            "batteries right now! I was using it for 3-4 hours per day (I have\n",
            "another mouse at work). Stayed switched off for the rest of the day in\n",
            "order to save batteries. Nice mouse when it works.. Mine is piece of\n",
            "junk, maybe the latest batch they've got was not up to standard. So I\n",
            "totally agree with Mr. Loftin.. And it's silver one in my case, not a\n",
            "black one I've ordered..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obb2ABDf1Seu"
      },
      "source": [
        "def tokenizer(text):\n",
        "    text = text.lower()\n",
        "    tokens = nltk.tokenize.word_tokenize(text)\n",
        "    tokens = [t for t in tokens if len(t) > 2]\n",
        "    tokens = [Word_lemmatizer.lemmatize(t) for t in tokens]\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "    return tokens"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8XChm_H1UJC"
      },
      "source": [
        "def tokens_to_vector(tokens, label):\n",
        "    x = np.zeros(len(word_map) + 1)\n",
        "    for t in tokens:\n",
        "        i = word_map[t]\n",
        "        x[i] +=1\n",
        "    x = x/x.sum()\n",
        "    x[-1] = label\n",
        "    return x"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O94Z63Kb7feX",
        "outputId": "366665ec-591c-4d6e-e7c3-dde7f595c9ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Word_lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(\"stop words list: \\n\")\n",
        "print(\"\\n\".join(textwrap.wrap(\", \".join(stop_words),70)))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stop word list: \n",
            "\n",
            "into, through, weren't, y, him, o, all, her, these, its, yourself,\n",
            "theirs, before, very, doing, mightn't, aren't, am, a, isn, about,\n",
            "over, she's, doesn't, shan't, more, while, doesn, couldn't, against,\n",
            "d, our, after, on, when, hasn't, such, that, needn't, yours, mustn,\n",
            "isn't, you'd, my, ll, hasn, hadn't, shouldn, ourselves, was, did,\n",
            "once, too, further, no, hadn, who, in, because, under, those, not,\n",
            "it's, by, than, yourselves, m, mightn, own, or, just, needn, can,\n",
            "weren, how, until, wouldn, does, she, that'll, above, down, won't, be,\n",
            "having, with, being, you've, you'll, ain, is, been, has, haven't, the,\n",
            "don't, mustn't, their, themselves, i, t, out, there, didn, here,\n",
            "where, itself, should've, what, will, this, himself, myself, and,\n",
            "haven, at, couldn, up, ve, re, shouldn't, during, wasn, were, they,\n",
            "but, between, won, as, most, s, do, ma, then, are, any, off, for, now,\n",
            "herself, same, ours, it, don, to, which, shan, didn't, he, we, of,\n",
            "aren, had, other, wouldn't, few, again, each, have, wasn't, only, so,\n",
            "you, your, whom, below, an, if, from, his, why, should, me, hers,\n",
            "some, both, nor, you're, them\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1mx3LTY2ncW",
        "outputId": "a0bee5ac-e186-49f9-eb19-6d8141dfbafb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word_map = {}\n",
        "current_index = 0\n",
        "positive_tokenized = []\n",
        "negative_tokenized = []\n",
        "\n",
        "for review in positive_reviews:\n",
        "    tokens = tokenizer(review)\n",
        "    positive_tokenized.append(tokens)\n",
        "    for token in tokens:\n",
        "        if token not in word_map:\n",
        "            word_map[token] = current_index\n",
        "            current_index +=1\n",
        "            \n",
        "for review in negative_reviews:\n",
        "    tokens = tokenizer(review)\n",
        "    negative_tokenized.append(tokens)\n",
        "    for token in tokens:\n",
        "        if token not in word_map:\n",
        "            word_map[token] = current_index\n",
        "            current_index +=1\n",
        "\n",
        "print(\"Total number of words in corpus: \",len(word_map))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words in corpus:  11293\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArX60XaW2qsi"
      },
      "source": [
        "N = len(negative_tokenized) + len(negative_tokenized)\n",
        "data = np.zeros((N,len(word_map) + 1))\n",
        "i = 0\n",
        "for tokens in positive_tokenized:\n",
        "    xy = tokens_to_vector(tokens, 1)\n",
        "    data[i,:] = xy\n",
        "    i +=1\n",
        "\n",
        "for tokens in negative_tokenized:\n",
        "    xy = tokens_to_vector(tokens, 0)\n",
        "    data[i,:] = xy\n",
        "    i +=1\n",
        "\n",
        "X = data[:,:-1]\n",
        "Y = data[:,-1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, Y, test_size=0.05, random_state=42)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Corm3Cq8SV8",
        "outputId": "41747b9f-f112-46bf-a53e-cffa27a186fc"
      },
      "source": [
        "LR_model = LogisticRegression()\n",
        "RF_model =  RandomForestClassifier()\n",
        "AB_model =  AdaBoostClassifier()\n",
        "NB_model = MultinomialNB()\n",
        "SVM_model = svm.LinearSVC()\n",
        "\n",
        "\n",
        "LR_model.fit(X_train, y_train)\n",
        "RF_model.fit(X_train, y_train)\n",
        "AB_model.fit(X_train, y_train)\n",
        "NB_model.fit(X_train, y_train)\n",
        "SVM_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "print(\"Logistic Regression classfifcation score: \", LR_model.score(X_test,y_test))\n",
        "print(\"Random Forest classfifcation score: \", RF_model.score(X_test,y_test))\n",
        "print(\"AdaBoost classfifcation score: \", AB_model.score(X_test,y_test))\n",
        "print(\"MultinomialNB classfifcation score: \", NB_model.score(X_test,y_test))\n",
        "print(\"SVM classfifcation score: \", SVM_model.score(X_test,y_test))\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression classfifcation score:  0.77\n",
            "Random Forest classfifcation score:  0.85\n",
            "AdaBoost classfifcation score:  0.7\n",
            "MultinomialNB classfifcation score:  0.78\n",
            "SVM classfifcation score:  0.78\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgh2vNSb8Siz",
        "outputId": "4d213408-cda1-4b2d-a4d8-52bcdf876d71"
      },
      "source": [
        "treshold = 0.5\n",
        "\n",
        "for word, index in word_map.items():\n",
        "    weight = LR_model.coef_[0][index]\n",
        "    if weight > treshold:\n",
        "      pos_words.append(word)\n",
        "    elif weight < -treshold:\n",
        "      neg_words.append(word)\n",
        "\n",
        "\n",
        "print(\"Positive words: \\n\")\n",
        "print(\"\\n\".join(textwrap.wrap(\", \".join(pos_words),70))+\"\\n\")  \n",
        "print(\"Negative words: \\n\")\n",
        "print(\"\\n\".join(textwrap.wrap(\", \".join(neg_words),70)))    "
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive words: \n",
            "\n",
            "price, 've, ha, love, easy, use, used, lot, memory, small, happy,\n",
            "recommend, well, good, sound, quality, problem, cable, little, paper,\n",
            "excellent, great, using, like, speaker, also, perfect, need, best,\n",
            "fast, pretty, comfortable, highly, price, 've, ha, love, easy, use,\n",
            "used, lot, memory, small, happy, recommend, well, good, sound,\n",
            "quality, problem, cable, little, paper, excellent, great, using, like,\n",
            "speaker, also, perfect, need, best, fast, pretty, comfortable, highly\n",
            "\n",
            "Negative words: \n",
            "\n",
            "n't, even, get, wa, item, could, month, unit, would, buy, first, time,\n",
            "week, back, bad, two, try, worked, doe, money, thing, support, tried,\n",
            "poor, returned, return, warranty, waste, customer, refund, n't, even,\n",
            "get, wa, item, could, month, unit, would, buy, first, time, week,\n",
            "back, bad, two, try, worked, doe, money, thing, support, tried, poor,\n",
            "returned, return, warranty, waste, customer, refund\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}