{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPi8hhxFC92u4QmGpHovflc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dvircohen0/NLP/blob/main/semantic_analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ouB6BnB7GbD",
        "outputId": "64e92612-bab0-44c5-8822-f2b15eada03e"
      },
      "source": [
        "import nltk\r\n",
        "import numpy as np\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.neural_network import MLPClassifier\r\n",
        "from sklearn import svm\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Corm3Cq8SV8",
        "outputId": "aced0159-3d05-42a4-f5d7-7e008087c3be"
      },
      "source": [
        "Word_lemmatizer = WordNetLemmatizer()\r\n",
        "stop_words = set(stopwords.words('english'))\r\n",
        "\r\n",
        "positive_reviews =  BeautifulSoup(open(\"positive.review\").read())\r\n",
        "positive_reviews =  positive_reviews.findAll('review_text')\r\n",
        "\r\n",
        "negative_reviews =  BeautifulSoup(open(\"negative.review\").read())\r\n",
        "negative_reviews =  negative_reviews.findAll('review_text')\r\n",
        "\r\n",
        "np.random.shuffle(positive_reviews)\r\n",
        "positive_reviews = positive_reviews[:len(negative_reviews)]\r\n",
        "\r\n",
        "def tokenizer(text):\r\n",
        "    text = text.lower()\r\n",
        "    tokens = nltk.tokenize.word_tokenize(text)\r\n",
        "    tokens = [t for t in tokens if len(t) > 2]\r\n",
        "    tokens = [Word_lemmatizer.lemmatize(t) for t in tokens]\r\n",
        "    tokens = [t for t in tokens if t not in stop_words]\r\n",
        "    return tokens\r\n",
        "    \r\n",
        "\r\n",
        "word_map = {}\r\n",
        "current_index = 0\r\n",
        "positive_tokenized = []\r\n",
        "negative_tokenized = []\r\n",
        "\r\n",
        "for review in positive_reviews:\r\n",
        "    tokens = tokenizer(review.text)\r\n",
        "    positive_tokenized.append(tokens)\r\n",
        "    for token in tokens:\r\n",
        "        if token not in word_map:\r\n",
        "            word_map[token] = current_index\r\n",
        "            current_index +=1\r\n",
        "            \r\n",
        "for review in negative_reviews:\r\n",
        "    tokens = tokenizer(review.text)\r\n",
        "    negative_tokenized.append(tokens)\r\n",
        "    for token in tokens:\r\n",
        "        if token not in word_map:\r\n",
        "            word_map[token] = current_index\r\n",
        "            current_index +=1\r\n",
        "\r\n",
        "def tokens_to_vector(tokens, label):\r\n",
        "    x = np.zeros(len(word_map) + 1)\r\n",
        "    for t in tokens:\r\n",
        "        i = word_map[t]\r\n",
        "        x[i] +=1\r\n",
        "    x = x/x.sum()\r\n",
        "    x[-1] = label\r\n",
        "    return x\r\n",
        "\r\n",
        "N = len(negative_tokenized) + len(negative_tokenized)\r\n",
        "data = np.zeros((N,len(word_map) + 1))\r\n",
        "i = 0\r\n",
        "for tokens in positive_tokenized:\r\n",
        "    xy = tokens_to_vector(tokens, 1)\r\n",
        "    data[i,:] = xy\r\n",
        "    i +=1\r\n",
        "\r\n",
        "for tokens in negative_tokenized:\r\n",
        "    xy = tokens_to_vector(tokens, 0)\r\n",
        "    data[i,:] = xy\r\n",
        "    i +=1\r\n",
        "\r\n",
        "X = data[:,:-1]\r\n",
        "Y = data[:,-1]\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(\r\n",
        "    X, Y, test_size=0.05, random_state=42)\r\n",
        "\r\n",
        "\r\n",
        "LR_model = LogisticRegression()\r\n",
        "RF_model =  RandomForestClassifier()\r\n",
        "AB_model =  AdaBoostClassifier()\r\n",
        "NB_model = MultinomialNB()\r\n",
        "SVM_model = svm.LinearSVC()\r\n",
        "\r\n",
        "\r\n",
        "LR_model.fit(X_train, y_train)\r\n",
        "RF_model.fit(X_train, y_train)\r\n",
        "AB_model.fit(X_train, y_train)\r\n",
        "NB_model.fit(X_train, y_train)\r\n",
        "SVM_model.fit(X_train, y_train)\r\n",
        "\r\n",
        "\r\n",
        "print(\"Logistic Regression classfifcation score: \", LR_model.score(X_test,y_test))\r\n",
        "print(\"Random Forest classfifcation score: \", RF_model.score(X_test,y_test))\r\n",
        "print(\"AdaBoost classfifcation score: \", AB_model.score(X_test,y_test))\r\n",
        "print(\"MultinomialNB classfifcation score: \", NB_model.score(X_test,y_test))\r\n",
        "print(\"SVM classfifcation score: \", SVM_model.score(X_test,y_test))\r\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression classfifcation score:  0.75\n",
            "Random Forest classfifcation score:  0.84\n",
            "AdaBoost classfifcation score:  0.75\n",
            "MultinomialNB classfifcation score:  0.82\n",
            "SVM classfifcation score:  0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgh2vNSb8Siz",
        "outputId": "78db1e88-2aea-4ebc-9da7-b54bcb330c67"
      },
      "source": [
        "treshold = 0.5\r\n",
        "for word, index in word_map.items():\r\n",
        "    weight = LR_model.coef_[0][index]\r\n",
        "    if weight > treshold or weight < -treshold:\r\n",
        "        print(word, weight)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "perfect 0.8371926895217009\n",
            "n't -1.9020767262257412\n",
            "quality 1.1589759348517612\n",
            "sound 0.9012381247228022\n",
            "fast 0.8191768101262733\n",
            "wa -1.2082294519185264\n",
            "could -0.5058744251334883\n",
            "even -0.735701278876155\n",
            "doe -1.063260700423381\n",
            "happy 0.5333140262372946\n",
            "would -0.6735875478115538\n",
            "recommend 0.6251694842873292\n",
            "good 1.7891190674046458\n",
            "well 0.8523617569780264\n",
            "small 0.5887351701414605\n",
            "love 0.9121671291662387\n",
            "time -0.5991885393578545\n",
            "ha 0.628333378578018\n",
            "comfortable 0.5284075792835006\n",
            "use 1.5370817041503742\n",
            "item -0.9181882400543532\n",
            "month -0.6692900372643024\n",
            "lot 0.5569560102313887\n",
            "paper 0.5607935080727208\n",
            "price 2.26569697623441\n",
            "great 3.4999042624686427\n",
            "little 0.6069733968480449\n",
            "easy 1.3376612400721475\n",
            "unit -0.6473419994020221\n",
            "'ve 0.5230796404963847\n",
            "need 0.519577576534241\n",
            "get -1.0742681758055401\n",
            "like 0.5772794838896115\n",
            "two -0.5418838602617079\n",
            "back -1.4299655143958\n",
            "speaker 0.7722764757231644\n",
            "thing -0.8251131547978069\n",
            "problem 0.5543412472369411\n",
            "bad -0.5739750080011543\n",
            "cable 0.5681156600463296\n",
            "buy -0.9282317879601224\n",
            "money -0.9070627321201428\n",
            "best 0.9847562279048496\n",
            "memory 0.8189741498312874\n",
            "used 1.0002330996464799\n",
            "pretty 0.5433654776755645\n",
            "tried -0.7022809103729933\n",
            "worked -0.7420066560873593\n",
            "excellent 1.082766793489932\n",
            "first -0.6460168295328037\n",
            "warranty -0.5104387969873663\n",
            "try -0.5228516483536866\n",
            "highly 0.865305637976129\n",
            "customer -0.60122783169626\n",
            "waste -0.8788181683495654\n",
            "returned -0.6789990708572036\n",
            "return -0.9925563058583419\n",
            "week -0.5410463993479158\n",
            "support -0.7149814998508854\n",
            "refund -0.5176341421251486\n",
            "poor -0.6519789703583342\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}